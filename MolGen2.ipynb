{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvvJaZpb2qcAMhhQAU/Uf3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robinrb7/MolGen/blob/main/MolGen2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIAQ340sN1c9"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "import shap\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, roc_auc_score, f1_score,\n",
        "                             precision_recall_curve, confusion_matrix,\n",
        "                             classification_report, roc_curve, auc)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/1144.csv\")\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Drop non-numeric columns (e.g., \"Sequence ID\")\n",
        "df = df.drop(columns=[\"Sequence ID\"])\n",
        "\n",
        "# Normalize the 'Z Scale Electronic' column\n",
        "scaler = StandardScaler()\n",
        "df[\"Z Scale Electronic\"] = scaler.fit_transform(df[[\"Z-Scale Electronic\"]])\n",
        "df[\"Disulfide Bond Estimate\"] = scaler.fit_transform(df[[\"Disulfide Bond Estimation\"]])\n",
        "\n",
        "X = df.drop(columns=[\"Label\", \"Molecular Weight\",'AAC_A','AAC_D','AAC_F','AAC_G','AAC_H','Tiny Residue %','Large Residue %','Nonpolar Residue %','Fraction Small Hydrophobic',\n",
        "                     'Bulkiness','Isoelectric Point', 'Aliphatic Index','GRAVY (Hydrophobicity)','Fraction Large Hydrophobic','Cysteine Cluster Ratio',\n",
        "                     'Alpha-Helix %','Beta-Sheet %','Coil %','Z-Scale Electronic','Disulfide Bond Estimation'])\n",
        "y = df[\"Label\"]\n",
        "\n",
        "# Train (70%) - Validation (15%) - Test (15%) Split\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, stratify=y_train_val, random_state=42)\n",
        "\n",
        "# Convert data into DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"auc\",\n",
        "        \"booster\": \"gbtree\",\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 2),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0, 3),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 10, 20),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.8),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 0.8),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 5, 50),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 5, 50),\n",
        "        \"tree_method\": \"gpu_hist\" if torch.cuda.is_available() else \"hist\"\n",
        "    }\n",
        "\n",
        "    num_boost_round = trial.suggest_int(\"num_boost_round\", 200, 1000, step=100)\n",
        "\n",
        "    # Perform Cross-validation\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=2000,\n",
        "        nfold=5,\n",
        "        stratified=True,\n",
        "        metrics=\"auc\",\n",
        "        as_pandas=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Extract best training and validation AUCs\n",
        "    best_valid_auc = cv_results[\"test-auc-mean\"].max()  # Validation AUC\n",
        "    best_train_auc = cv_results[\"train-auc-mean\"].max()  # Training AUC\n",
        "\n",
        "    # Store values in Optuna trial attributes for later use\n",
        "    trial.set_user_attr(\"train_auc\", best_train_auc)\n",
        "    trial.set_user_attr(\"valid_auc\", best_valid_auc)\n",
        "\n",
        "    return best_valid_auc  # Optimize for validation AUC\n",
        "\n",
        "# Run Optuna hyperparameter tuning\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Train the best model\n",
        "xg_params = study.best_params\n",
        "xg_model = xgb.XGBClassifier(**xg_params, eval_metric='logloss')\n",
        "\n",
        "# Train the model\n",
        "xg_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predictions\n",
        "y_pred = xg_model.predict(X_test)\n",
        "y_pred_proba = xg_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Precision, Recall, Sensitivity & Specificity\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "precision = report[\"1\"][\"precision\"]\n",
        "recall = report[\"1\"][\"recall\"]\n",
        "specificity = report[\"0\"][\"recall\"]\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "print(f\"‚úÖ AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"‚úÖ F1 Score: {f1:.4f}\")\n",
        "print(f\"‚úÖ Precision: {precision:.4f}\")\n",
        "print(f\"‚úÖ Recall (Sensitivity): {recall:.4f}\")\n",
        "print(f\"‚úÖ Specificity: {specificity:.4f}\")\n",
        "print(f\"‚è≥ Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "# Feature Importance with SHAP\n",
        "explainer = shap.Explainer(xg_model)\n",
        "shap_values = explainer(X_test)\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "oN2nx6y4OuUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "model = xgb.train(xg_params, dtrain, num_boost_round=100)\n",
        "xgb.plot_importance(model)"
      ],
      "metadata": {
        "id": "CBLJmEpaOwYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ Plot Confusion Matrix\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pFZM2WHTOzbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Function to plot learning curves\n",
        "def plot_learning_curve(estimator, X, y, cv=5, scoring='accuracy'):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, train_sizes=np.linspace(0.1, 1.0, 10)\n",
        "    )\n",
        "\n",
        "    # Compute mean and standard deviation\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='red', label='Training Score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='red')\n",
        "    plt.plot(train_sizes, test_mean, 'o-', color='green', label='Test Score')\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
        "\n",
        "    plt.xlabel(\"Training Examples\")\n",
        "    plt.ylabel(scoring.title())\n",
        "    plt.title(\"Learning Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Plot learning curve for the trained model\n",
        "plot_learning_curve(xg_model, X_train, y_train, cv=5, scoring='accuracy')"
      ],
      "metadata": {
        "id": "SaDJlIiDO1LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Optuna study results\n",
        "trials = study.trials_dataframe()\n",
        "epochs = range(1, len(trials) + 1)  # Number of trials as x-axis\n",
        "\n",
        "# Extract Training & Validation AUCs\n",
        "train_auc_scores = [t.user_attrs.get(\"train_auc\", None) for t in study.trials]\n",
        "valid_auc_scores = [t.user_attrs.get(\"valid_auc\", None) for t in study.trials]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Plot validation AUC (Orange Line)\n",
        "plt.plot(epochs, valid_auc_scores, marker=\"o\", linestyle=\"-\", color=\"orange\", label=\"Validation AUC\")\n",
        "\n",
        "# Plot training AUC (Blue Line)\n",
        "plt.plot(epochs, train_auc_scores, marker=\"o\", linestyle=\"-\", color=\"blue\", label=\"Training AUC\")\n",
        "\n",
        "plt.xlabel(\"Trials (Epochs)\")\n",
        "plt.ylabel(\"AUC Score\")\n",
        "plt.title(\"Optuna Trials: Training vs Validation AUC\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DFEZ041vO3Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract training and validation loss from evals_result\n",
        "evals_result = xg_model.evals_result()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot training loss\n",
        "plt.plot(evals_result[\"validation_0\"][\"logloss\"], label=\"Training Loss\", linestyle=\"-\", marker=\"o\", color=\"red\")\n",
        "\n",
        "# Plot validation loss\n",
        "#plt.plot(evals_result[\"validation_1\"][\"logloss\"], label=\"Validation Loss\", linestyle=\"-\", marker=\"s\", color=\"blue\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Log Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y4oUb7PfO5GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Example: List of (model_name, true_labels, predicted_probabilities)\n",
        "models = [\n",
        "    (\"XGBoost\", y_test, xg_model.predict_proba(X_test)[:, 1]),\n",
        "    # Add other models here in the same format\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "for model_name, y_true, y_pred_proba in models:\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc:.4f})\", linewidth=2)\n",
        "\n",
        "# Fill the area under the curve\n",
        "plt.fill_between(fpr, tpr, color=\"pink\", alpha=0.3)\n",
        "\n",
        "# Diagonal reference line\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", alpha=0.7)\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "plt.title(\"ROC Curve (TPR vs FPR)\", fontsize=14)\n",
        "\n",
        "# No grid lines\n",
        "plt.gca().spines[\"top\"].set_visible(False)\n",
        "plt.gca().spines[\"right\"].set_visible(False)\n",
        "plt.gca().spines[\"left\"].set_linewidth(1)\n",
        "plt.gca().spines[\"bottom\"].set_linewidth(1)\n",
        "\n",
        "# Legend\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B9WfjEZjO7Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£ Plot Precision-Recall Curve\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(recall_vals, precision_vals, marker='.')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NYnDdWpAO-WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(xg_model, \"xg_model.pkl\")\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(\"xg_model.pkl\")\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Load trained model\n",
        "model_path = \"/content/xg_model.pkl\"  # Replace with actual path\n",
        "loaded_model = joblib.load(model_path)\n",
        "\n",
        "# Load new data for prediction\n",
        "df_new = pd.read_csv(\"/content/drive/MyDrive/to_predict.csv\")  # Replace with actual file path\n",
        "\n",
        "# Store Sequence ID and Molecular Weight before dropping\n",
        "sequence_ids = df_new[\"Sequence ID\"] if \"Sequence ID\" in df_new.columns else None\n",
        "mol_weights = df_new[\"Molecular Weight\"] if \"Molecular Weight\" in df_new.columns else None\n",
        "\n",
        "# Drop unnecessary columns for prediction\n",
        "df_new = df_new.drop(columns=[\"Sequence ID\", \"Molecular Weight\"], errors=\"ignore\")\n",
        "\n",
        "# Predict labels\n",
        "y_new_pred = loaded_model.predict(df_new)\n",
        "\n",
        "# Add predictions as a new column\n",
        "df_new[\"Label\"] = y_new_pred\n",
        "\n",
        "# Reattach Sequence ID and Molecular Weight if they were originally present\n",
        "if sequence_ids is not None:\n",
        "    df_new.insert(0, \"Sequence ID\", sequence_ids)\n",
        "if mol_weights is not None:\n",
        "    df_new[\"Molecular Weight\"] = mol_weights\n",
        "\n",
        "# Save to a new CSV file\n",
        "output_path = \"/content/drive/MyDrive/xg_prediction.csv\"\n",
        "df_new.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Predictions saved to {output_path}\")\n",
        "\n",
        "count_ones = df_new['Label'].sum()\n",
        "print(count_ones)\n",
        "\n",
        "# Filter rows where Label is 1\n",
        "df_label_1 = df_new[df_new[\"Label\"] == 1]\n",
        "\n",
        "# Extract Sequence IDs\n",
        "sequence_ids_label_1 = df_label_1[\"Sequence ID\"]\n",
        "print(sequence_ids_label_1)"
      ],
      "metadata": {
        "id": "yQ43vU-cPAac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import shap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/total_antibody.csv\")\n",
        "\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle dataset\n",
        "\n",
        "# Drop non-numeric columns\n",
        "df = df.drop(columns=[\"Sequence ID\"])\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(columns=[\"Label\", \"Molecular Weight\"])\n",
        "y = df[\"Label\"]\n",
        "\n",
        "# Remove highly correlated features\n",
        "corr_matrix = X.corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]\n",
        "X = X.drop(columns=high_corr_features)\n",
        "\n",
        "# Train (70%) - Validation (15%) - Test (15%) Split\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, stratify=y_train_val, random_state=42)\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 250, step=50),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 10, 30),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 5, 20),\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
        "        \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 20, 50, step=5),\n",
        "        \"min_impurity_decrease\": trial.suggest_float(\"min_impurity_decrease\", 0.005, 0.05),\n",
        "        \"ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 0.005, 0.1),\n",
        "        \"class_weight\": \"balanced\",\n",
        "    }\n",
        "\n",
        "    model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Compute Training AUC\n",
        "    y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
        "    train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "\n",
        "    # Compute Validation AUC\n",
        "    y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
        "    val_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "\n",
        "    # Compute Training Log Loss\n",
        "    train_loss = log_loss(y_train, y_train_pred_proba)\n",
        "\n",
        "    # Compute Validation Log Loss\n",
        "    val_loss = log_loss(y_val, y_val_pred_proba)\n",
        "\n",
        "    # Store metrics inside the trial\n",
        "    trial.set_user_attr(\"train_auc\", train_auc)\n",
        "    trial.set_user_attr(\"valid_auc\", val_auc)\n",
        "    trial.set_user_attr(\"train_loss\", train_loss)\n",
        "    trial.set_user_attr(\"valid_loss\", val_loss)\n",
        "\n",
        "    return val_auc  # Optimize for validation AUC\n",
        "\n",
        "\n",
        "# Run Optuna tuning\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Train the best Random Forest model\n",
        "best_params = study.best_params\n",
        "rf_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on validation set (Early stopping checkpoint)\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "y_val_proba = rf_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"üîç Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"üîç Validation AUC Score: {val_roc_auc:.4f}\")\n",
        "print(f\"üîç Validation F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "# If validation AUC is below a threshold, stop training and do not test\n",
        "if val_roc_auc < 0.7:  # Early stopping condition\n",
        "    print(\"‚ö†Ô∏è Early stopping: Model did not reach required AUC on validation set!\")\n",
        "else:\n",
        "    # Final evaluation on test set\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    test_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"‚úÖ Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"‚úÖ Test AUC Score: {test_roc_auc:.4f}\")\n",
        "    print(f\"‚úÖ Test F1 Score: {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "5X5PEjdpPCK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Plot\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w1bp9LuXPECN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot learning curves\n",
        "def plot_learning_curve(estimator, X, y, cv=5, scoring='accuracy'):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, train_sizes=np.linspace(0.1, 1.0, 10)\n",
        "    )\n",
        "\n",
        "    # Compute mean and standard deviation\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='red', label='Training Score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='red')\n",
        "    plt.plot(train_sizes, test_mean, 'o-', color='green', label='Test Score')\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
        "\n",
        "    plt.xlabel(\"Training Examples\")\n",
        "    plt.ylabel(scoring.title())\n",
        "    plt.title(\"Learning Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot learning curve for the trained model\n",
        "plot_learning_curve(rf_model, X_train, y_train, cv=5, scoring='accuracy')"
      ],
      "metadata": {
        "id": "MXU9ezZCPGHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Optuna study results\n",
        "trials = study.trials_dataframe()\n",
        "epochs = range(1, len(trials) + 1)  # Number of trials as x-axis\n",
        "\n",
        "# Extract Training & Validation AUCs\n",
        "train_auc_scores = [t.user_attrs.get(\"train_auc\", None) for t in study.trials]\n",
        "valid_auc_scores = [t.user_attrs.get(\"valid_auc\", None) for t in study.trials]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Plot validation AUC (Orange Line)\n",
        "plt.plot(epochs, valid_auc_scores, marker=\"o\", linestyle=\"-\", color=\"orange\", label=\"Validation AUC\")\n",
        "\n",
        "# Plot training AUC (Blue Line)\n",
        "plt.plot(epochs, train_auc_scores, marker=\"o\", linestyle=\"-\", color=\"blue\", label=\"Training AUC\")\n",
        "\n",
        "plt.xlabel(\"Trials (Epochs)\")\n",
        "plt.ylabel(\"AUC Score\")\n",
        "plt.title(\"Optuna Trials: Training vs Validation AUC\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eRnzEWYGPH2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract log loss values from trials\n",
        "train_loss_scores = [t.user_attrs.get(\"train_loss\", None) for t in study.trials]\n",
        "valid_loss_scores = [t.user_attrs.get(\"valid_loss\", None) for t in study.trials]\n",
        "\n",
        "# Filter out None values\n",
        "train_loss_scores = [loss for loss in train_loss_scores if loss is not None]\n",
        "valid_loss_scores = [loss for loss in valid_loss_scores if loss is not None]\n",
        "\n",
        "epochs = range(1, len(valid_loss_scores) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot training loss\n",
        "plt.plot(epochs, train_loss_scores, label=\"Training Loss\", linestyle=\"-\", marker=\"o\", color=\"red\")\n",
        "\n",
        "# Plot validation loss\n",
        "plt.plot(epochs, valid_loss_scores, label=\"Validation Loss\", linestyle=\"-\", marker=\"s\", color=\"blue\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Log Loss\")\n",
        "plt.title(\"Random Forest: Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eXwUCADWPIih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "# Random Forest Model ROC Curve\n",
        "y_test_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_test_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f\"Random Forest (AUC = {roc_auc:.4f})\", linewidth=2)\n",
        "\n",
        "# Fill the area under the curve\n",
        "plt.fill_between(fpr, tpr, color=\"pink\", alpha=0.3)\n",
        "\n",
        "# Diagonal reference line\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", alpha=0.7)\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "plt.title(\"ROC Curve: Random Forest\", fontsize=14)\n",
        "\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M5yc2OpLPKd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_test_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(recall_vals, precision_vals, marker='.', label=\"Random Forest\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve: Random Forest\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sb363k3tPMHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(rf_model, \"rf_model.pkl\")\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(\"rf_model.pkl\")\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Load trained model\n",
        "model_path = \"/content/rf_model.pkl\"  # Replace with actual path\n",
        "loaded_model = joblib.load(model_path)\n",
        "\n",
        "# Load new data for prediction\n",
        "df_new = pd.read_csv(\"/content/drive/MyDrive/to_predict.csv\")  # Replace with actual file path\n",
        "\n",
        "# Store Sequence ID and Molecular Weight before dropping\n",
        "sequence_ids = df_new[\"Sequence ID\"] if \"Sequence ID\" in df_new.columns else None\n",
        "mol_weights = df_new[\"Molecular Weight\"] if \"Molecular Weight\" in df_new.columns else None\n",
        "\n",
        "# Drop unnecessary columns for prediction\n",
        "df_new = df_new.drop(columns=[\"Sequence ID\", \"Molecular Weight\"], errors=\"ignore\")\n",
        "\n",
        "# Get the training data feature names from the loaded model\n",
        "training_feature_names = loaded_model.feature_names_in_\n",
        "\n",
        "# Ensure the new data has the same columns as the training data, in the same order\n",
        "df_new = df_new.reindex(columns=training_feature_names)\n",
        "\n",
        "\n",
        "# Predict labels\n",
        "y_new_pred = loaded_model.predict(df_new)\n",
        "\n",
        "# Add predictions as a new column\n",
        "df_new[\"Label\"] = y_new_pred\n",
        "\n",
        "# Reattach Sequence ID and Molecular Weight if they were originally present\n",
        "if sequence_ids is not None:\n",
        "    df_new.insert(0, \"Sequence ID\", sequence_ids)\n",
        "if mol_weights is not None:\n",
        "    df_new[\"Molecular Weight\"] = mol_weights\n",
        "\n",
        "# Save to a new CSV file\n",
        "output_path = \"/content/drive/MyDrive/prediction.csv\"\n",
        "df_new.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Predictions saved to {output_path}\")\n",
        "\n",
        "count_ones = df_new['Label'].sum()\n",
        "print(count_ones)\n",
        "# Filter rows where Label is 1\n",
        "df_label_1 = df_new[df_new[\"Label\"] == 1]\n",
        "\n",
        "# Extract Sequence IDs\n",
        "sequence_ids_label_1 = df_label_1[\"Sequence ID\"]\n",
        "sequence_ids_label_1"
      ],
      "metadata": {
        "id": "KumQ1-ySPPJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, roc_curve, auc, precision_recall_curve, confusion_matrix, log_loss\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/1165.csv\")\n",
        "\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle dataset\n",
        "\n",
        "# Drop non-numeric columns\n",
        "df = df.drop(columns=[\"Sequence ID\"])\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(columns=[\"Label\", \"Molecular Weight\"])\n",
        "y = df[\"Label\"]\n",
        "\n",
        "# Train-test split (80-20)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, stratify=y_train_val, random_state=42)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 4, 20),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 10),\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
        "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
        "        \"ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 0.0001, 0.02)\n",
        "    }\n",
        "\n",
        "    model = DecisionTreeClassifier(**params, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on training and validation sets\n",
        "    y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
        "    y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    # Compute AUC scores\n",
        "    train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "    valid_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "\n",
        "    # Compute Log Loss scores\n",
        "    train_loss = log_loss(y_train, model.predict_proba(X_train))\n",
        "    valid_loss = log_loss(y_val, model.predict_proba(X_val))\n",
        "\n",
        "    # Store AUC and Log Loss values in Optuna's trial attributes\n",
        "    trial.set_user_attr(\"train_auc\", train_auc)\n",
        "    trial.set_user_attr(\"valid_auc\", valid_auc)\n",
        "    trial.set_user_attr(\"train_loss\", train_loss)\n",
        "    trial.set_user_attr(\"valid_loss\", valid_loss)\n",
        "\n",
        "    return valid_auc  # Optimize based on validation AUC\n",
        "\n",
        "# Run Optuna tuning\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=70)\n",
        "\n",
        "# Train the best model\n",
        "dtc_params = study.best_params\n",
        "dtc_model = DecisionTreeClassifier(**dtc_params, random_state=42)\n",
        "dtc_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = dtc_model.predict(X_test)\n",
        "y_pred_proba = dtc_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "print(f\"‚úÖ AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"‚úÖ F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "Z3T-B0FLPQ4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ Confusion Matrix\n",
        "plt.figure(figsize=(5, 4))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0HpLG2OAPTiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£ Learning Curve\n",
        "def plot_learning_curve(estimator, X, y, cv=5, scoring='accuracy'):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, train_sizes=np.linspace(0.1, 1.0, 10)\n",
        "    )\n",
        "\n",
        "    # Compute mean and standard deviation\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='red', label='Training Score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='red')\n",
        "    plt.plot(train_sizes, test_mean, 'o-', color='green', label='Test Score')\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
        "\n",
        "    plt.xlabel(\"Training Examples\")\n",
        "    plt.ylabel(scoring.title())\n",
        "    plt.title(\"Learning Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curve(dtc_model, X_train, y_train, cv=5, scoring='roc_auc')"
      ],
      "metadata": {
        "id": "S6TqNLZPPVKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trials = study.trials_dataframe()\n",
        "epochs = range(1, len(trials) + 1)  # Number of trials as x-axis\n",
        "\n",
        "# Extract Training & Validation AUCs\n",
        "train_auc_scores = [t.user_attrs.get(\"train_auc\", None) for t in study.trials]\n",
        "valid_auc_scores = [t.user_attrs.get(\"valid_auc\", None) for t in study.trials]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Plot validation AUC (Orange Line)\n",
        "plt.plot(epochs, valid_auc_scores, marker=\"o\", linestyle=\"-\", color=\"orange\", label=\"Validation AUC\")\n",
        "\n",
        "# Plot training AUC (Blue Line)\n",
        "plt.plot(epochs, train_auc_scores, marker=\"o\", linestyle=\"-\", color=\"blue\", label=\"Training AUC\")\n",
        "\n",
        "plt.xlabel(\"Trials (Epochs)\")\n",
        "plt.ylabel(\"AUC Score\")\n",
        "plt.title(\"Optuna Trials: Training vs Validation AUC\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ZhsSfi1PVvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract log loss values from trials\n",
        "train_loss_scores = [t.user_attrs.get(\"train_loss\", None) for t in study.trials]\n",
        "valid_loss_scores = [t.user_attrs.get(\"valid_loss\", None) for t in study.trials]\n",
        "\n",
        "# Filter out None values\n",
        "train_loss_scores = [loss for loss in train_loss_scores if loss is not None]\n",
        "valid_loss_scores = [loss for loss in valid_loss_scores if loss is not None]\n",
        "\n",
        "epochs = range(1, len(valid_loss_scores) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot training loss\n",
        "plt.plot(epochs, train_loss_scores, label=\"Training Loss\", linestyle=\"-\", marker=\"o\", color=\"red\")\n",
        "\n",
        "# Plot validation loss\n",
        "plt.plot(epochs, valid_loss_scores, label=\"Validation Loss\", linestyle=\"-\", marker=\"s\", color=\"blue\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Log Loss\")\n",
        "plt.title(\"Random Forest: Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8TAaFdHPXdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "# Random Forest Model ROC Curve\n",
        "y_test_pred_proba = dtc_model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_test_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f\"Random Forest (AUC = {roc_auc:.4f})\", linewidth=2)\n",
        "\n",
        "# Fill the area under the curve\n",
        "plt.fill_between(fpr, tpr, color=\"pink\", alpha=0.3)\n",
        "\n",
        "# Diagonal reference line\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", alpha=0.7)\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "plt.title(\"ROC Curve: Decision Tree\", fontsize=14)\n",
        "\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L44HcRVSPZTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_test_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(recall_vals, precision_vals, marker='.', label=\"Decision Tree\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve: Decision Tree\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E6_ciI58Pa1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(dtc_model, \"dtc_model.pkl\")\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(\"dtc_model.pkl\")\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Load trained model\n",
        "model_path = \"/content/dtc_model.pkl\"  # Replace with actual path\n",
        "loaded_model = joblib.load(model_path)\n",
        "\n",
        "# Load new data for prediction\n",
        "df_new = pd.read_csv(\"/content/drive/MyDrive/to_predict.csv\")  # Replace with actual file path\n",
        "\n",
        "# Store Sequence ID and Molecular Weight before dropping\n",
        "sequence_ids = df_new[\"Sequence ID\"] if \"Sequence ID\" in df_new.columns else None\n",
        "mol_weights = df_new[\"Molecular Weight\"] if \"Molecular Weight\" in df_new.columns else None\n",
        "\n",
        "# Drop unnecessary columns for prediction\n",
        "df_new = df_new.drop(columns=[\"Sequence ID\", \"Molecular Weight\"], errors=\"ignore\")\n",
        "\n",
        "# Predict labels\n",
        "y_new_pred = loaded_model.predict(df_new)\n",
        "\n",
        "# Add predictions as a new column\n",
        "df_new[\"Label\"] = y_new_pred\n",
        "\n",
        "# Reattach Sequence ID and Molecular Weight if they were originally present\n",
        "if sequence_ids is not None:\n",
        "    df_new.insert(0, \"Sequence ID\", sequence_ids)\n",
        "if mol_weights is not None:\n",
        "    df_new[\"Molecular Weight\"] = mol_weights\n",
        "\n",
        "# Save to a new CSV file\n",
        "output_path = \"/content/drive/MyDrive/dtc_prediction.csv\"\n",
        "df_new.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Predictions saved to {output_path}\")\n",
        "\n",
        "count_ones = df_new['Label'].sum()\n",
        "print(count_ones)\n",
        "\n",
        "# Filter rows where Label is 1\n",
        "df_label_1 = df_new[df_new[\"Label\"] == 1]\n",
        "\n",
        "# Extract Sequence IDs\n",
        "sequence_ids_label_1 = df_label_1[\"Sequence ID\"]\n",
        "print(sequence_ids_label_1)"
      ],
      "metadata": {
        "id": "hFlhZC_jPee1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/output2000.csv\")\n",
        "\n",
        "# Drop non-numeric columns\n",
        "df = df.drop(columns=[\"Sequence ID\"])\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(columns=[\"Label\", \"Molecular Weight\"])\n",
        "y = df[\"Label\"]\n",
        "\n",
        "# Standardize features (Na√Øve Bayes is sensitive to scale)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split (80-20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Optuna objective function\n",
        "def objective(trial):\n",
        "    var_smoothing = trial.suggest_loguniform(\"var_smoothing\", 1e-9, 1e-3)  # Smoothing factor\n",
        "\n",
        "    model = GaussianNB(var_smoothing=var_smoothing)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    return roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Run Optuna tuning\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Train the best model\n",
        "nb_params = study.best_params\n",
        "nb_model = GaussianNB(**nb_params)\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = nb_model.predict(X_test)\n",
        "y_pred_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "print(f\"‚úÖ AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"‚úÖ F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "Yqj7NH5PPfFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mzXdfxtPhkY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}